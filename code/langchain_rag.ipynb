{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from langchain_text_splitters import NLTKTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from IPython.display import Markdown as md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"AIzaSyBvjfo0zBAOggUjMmuu1H_O3olEgNf5itk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "chat_model = ChatGoogleGenerativeAI(google_api_key=key, \n",
    "                                   model=\"gemini-1.5-pro-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"1neural_network.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': '適用於 Microsoft 365 的 Microsoft® PowerPoint®', 'creator': '適用於 Microsoft 365 的 Microsoft® PowerPoint®', 'creationdate': '2024-02-19T15:15:35+08:00', 'title': '深度學習簡介', 'author': 'fhwang', 'moddate': '2024-02-19T15:15:35+08:00', 'source': '1neural_network.pdf', 'total_pages': 20, 'page': 0, 'page_label': '1'}, page_content='類神經網路基礎\\n王豐緒\\n銘傳大學資工系'),\n",
       " Document(metadata={'producer': '適用於 Microsoft 365 的 Microsoft® PowerPoint®', 'creator': '適用於 Microsoft 365 的 Microsoft® PowerPoint®', 'creationdate': '2024-02-19T15:15:35+08:00', 'title': '深度學習簡介', 'author': 'fhwang', 'moddate': '2024-02-19T15:15:35+08:00', 'source': '1neural_network.pdf', 'total_pages': 20, 'page': 1, 'page_label': '2'}, page_content='學習目標\\n• 理解類神經元的基本結構與運作方式\\n• 理解何謂Perceptron類神經網路\\n• 理解類神經的學習方式\\n• 理解類神經的訓練與測試過程\\n• 理解矩陣運算與類神經的關聯\\n2'),\n",
       " Document(metadata={'producer': '適用於 Microsoft 365 的 Microsoft® PowerPoint®', 'creator': '適用於 Microsoft 365 的 Microsoft® PowerPoint®', 'creationdate': '2024-02-19T15:15:35+08:00', 'title': '深度學習簡介', 'author': 'fhwang', 'moddate': '2024-02-19T15:15:35+08:00', 'source': '1neural_network.pdf', 'total_pages': 20, 'page': 2, 'page_label': '3'}, page_content='大綱\\n• 類神經元的結構與運作\\n• Perceptron類神經網路\\n• 學習方程式\\n• 學習速率的選擇\\n• 訓練與測試\\n• 訓練階段(backward process)\\n• 測試階段(Forward Process)\\n• 矩陣運算與類神經\\n• 小結\\n3'),\n",
       " Document(metadata={'producer': '適用於 Microsoft 365 的 Microsoft® PowerPoint®', 'creator': '適用於 Microsoft 365 的 Microsoft® PowerPoint®', 'creationdate': '2024-02-19T15:15:35+08:00', 'title': '深度學習簡介', 'author': 'fhwang', 'moddate': '2024-02-19T15:15:35+08:00', 'source': '1neural_network.pdf', 'total_pages': 20, 'page': 3, 'page_label': '4'}, page_content='大腦神經元(NEURONS)\\n• 神經元\\n• 大腦中的處理單元（1000億個，1011）\\n• 每個通過突觸與其他神經元相連（1千兆\\n個，1014）\\n• 具有可塑性和堅韌的操作性\\n• 經由學習，修改突觸強度\\n• 經由學習，建立新連接 (突觸，Synapse)\\n4\\n(source: Wiki)\\nneuron\\n決定是否觸發\\n突觸'),\n",
       " Document(metadata={'producer': '適用於 Microsoft 365 的 Microsoft® PowerPoint®', 'creator': '適用於 Microsoft 365 的 Microsoft® PowerPoint®', 'creationdate': '2024-02-19T15:15:35+08:00', 'title': '深度學習簡介', 'author': 'fhwang', 'moddate': '2024-02-19T15:15:35+08:00', 'source': '1neural_network.pdf', 'total_pages': 20, 'page': 4, 'page_label': '5'}, page_content='類神經元(ARTIFICIAL NEURONS)\\n• McCulloch and Pitts Neurons\\n5\\nh\\nx1\\nx2\\nxm\\nw1\\nw2\\nwm\\nO\\n…\\n-1\\n𝜃\\n偏值(bias)\\nneuron\\n在所有输入都是零的情况下需要\\n𝑜 = 𝑔(ℎ) = ቊ1 𝑖𝑓 ℎ > 0\\n0 𝑖𝑓 ℎ ≤ 0\\nℎ = \\u0dcd\\n𝑖\\n𝑊𝑖𝑋𝑖 − 𝜃\\n𝑔 : 激活函數 (activation function)\\nX: the input vector: [x1, x2, …, xm, -1]\\nW: the weight vector: [w1, w2, …, wm, 𝜃]\\n𝑜 = 𝑔(𝑊 ⊙ 𝑋)\\n⊙ : the inner product of 𝑊，𝑋'),\n",
       " Document(metadata={'producer': '適用於 Microsoft 365 的 Microsoft® PowerPoint®', 'creator': '適用於 Microsoft 365 的 Microsoft® PowerPoint®', 'creationdate': '2024-02-19T15:15:35+08:00', 'title': '深度學習簡介', 'author': 'fhwang', 'moddate': '2024-02-19T15:15:35+08:00', 'source': '1neural_network.pdf', 'total_pages': 20, 'page': 5, 'page_label': '6'}, page_content='PERCEPTRON類神經網路\\n• 最早的多神經元網路\\n• 啟發式學習法則\\n6\\n𝑥𝑖: 網路輸入\\n𝑦𝑗: 網路輸出\\n𝑡𝑗 : 真正的輸出\\n𝜂 : 學習速率 (learning rate)\\n𝑥𝑖 𝑦𝑗\\n△ 𝑤𝑖𝑗= −𝜂(𝑦𝑗 − 𝑡𝑗) ∙ 𝑥𝑖 WHY?\\n學習方程式\\n𝑦𝑗 − 𝑡𝑗\\n高估(𝑦𝑗 > 𝑡𝑗)\\n低估(𝑦𝑗 < 𝑡𝑗)\\n降低訊號強度z\\n增強訊號強度z\\n𝑧 = 𝑥𝑖𝑤𝑖𝑗目標\\n-\\n+\\n𝑥𝑖 𝑛𝑗\\n𝑤𝑖𝑗\\n𝑦𝑗\\n𝑧 = 𝑥𝑖𝑤𝑖𝑗'),\n",
       " Document(metadata={'producer': '適用於 Microsoft 365 的 Microsoft® PowerPoint®', 'creator': '適用於 Microsoft 365 的 Microsoft® PowerPoint®', 'creationdate': '2024-02-19T15:15:35+08:00', 'title': '深度學習簡介', 'author': 'fhwang', 'moddate': '2024-02-19T15:15:35+08:00', 'source': '1neural_network.pdf', 'total_pages': 20, 'page': 6, 'page_label': '7'}, page_content='學習速率的選擇\\n• 較小的 learning rate\\n• 學習較慢\\n• 較能容忍資料雜訊和資料不一致性\\n• 較大的 learning rate\\n• 較不穩定\\n• 實務上\\n• 可嘗試 0.1 < 𝜂 <0.4\\n7\\n△ 𝑤𝑖𝑗= −𝜂(𝑦𝑗 − 𝑡𝑗) ∙ 𝑥𝑖'),\n",
       " Document(metadata={'producer': '適用於 Microsoft 365 的 Microsoft® PowerPoint®', 'creator': '適用於 Microsoft 365 的 Microsoft® PowerPoint®', 'creationdate': '2024-02-19T15:15:35+08:00', 'title': '深度學習簡介', 'author': 'fhwang', 'moddate': '2024-02-19T15:15:35+08:00', 'source': '1neural_network.pdf', 'total_pages': 20, 'page': 7, 'page_label': '8'}, page_content=\"訓練(TRAIN)與測試(TEST)\\n• 訓練階段\\n• 將訓練數據x輸入到類神經網路中\\n並更新權重直到輸出正確答案y\\n• 測試階段\\n• 將測試數據x輸入到類神經網路中\\n並取得網路輸出y'\\n8\\n類神經網路\\n(W)x y\\n類神經網路\\n(W)x y'\"),\n",
       " Document(metadata={'producer': '適用於 Microsoft 365 的 Microsoft® PowerPoint®', 'creator': '適用於 Microsoft 365 的 Microsoft® PowerPoint®', 'creationdate': '2024-02-19T15:15:35+08:00', 'title': '深度學習簡介', 'author': 'fhwang', 'moddate': '2024-02-19T15:15:35+08:00', 'source': '1neural_network.pdf', 'total_pages': 20, 'page': 8, 'page_label': '9'}, page_content='訓練階段(BACKWARD PROCESS)\\n• 輸入訓練資料 : 𝐷(𝑀+1)×𝑁\\n• N筆 (M+1)-input 向量\\n• 加入偏值向量(-1)\\n• 輸出訓練資料 : 𝐿𝑂×𝑁\\n• 權重向量(Weight Matrix)\\n• 𝑊(𝑀+1)×𝑂\\n• 加入偏值權重(B)(bias weight): 𝐵1×𝑂\\n• 網路輸出Y : 𝑌𝑂×𝑁\\n• 總權重修正量矩陣: ∆𝑊𝑂×(𝑀+1)\\n9\\n𝑊𝑂×(𝑀+1)\\n𝑇 × 𝐷(𝑀+1)×𝑁= 𝐻𝑂×𝑁\\n𝑌𝑂×𝑁= 𝑔(𝐻𝑂×𝑁)\\n∆𝑊=−𝜂𝐷(𝑀+1)×𝑁 × (𝑌𝑂×𝑁 − 𝐿𝑂×𝑁)𝑇\\nB   \\nWM\\nO\\n1\\n-1\\nD\\nN\\nM LO\\nN\\n1\\nYO\\nN\\n𝑥𝑖 𝑦𝑗'),\n",
       " Document(metadata={'producer': '適用於 Microsoft 365 的 Microsoft® PowerPoint®', 'creator': '適用於 Microsoft 365 的 Microsoft® PowerPoint®', 'creationdate': '2024-02-19T15:15:35+08:00', 'title': '深度學習簡介', 'author': 'fhwang', 'moddate': '2024-02-19T15:15:35+08:00', 'source': '1neural_network.pdf', 'total_pages': 20, 'page': 9, 'page_label': '10'}, page_content='訓練階段(BACKWARD PROCESS)\\n10\\n−1\\nbias\\nD O\\nB   \\nWM\\nO\\n1\\nY O\\nN\\n-11\\nD\\nN\\nM\\nN\\nLO\\n△ 𝒘𝒊𝒋= −𝜼(𝒚𝒋 − 𝒕𝒋) ∙ 𝒙𝒊'),\n",
       " Document(metadata={'producer': '適用於 Microsoft 365 的 Microsoft® PowerPoint®', 'creator': '適用於 Microsoft 365 的 Microsoft® PowerPoint®', 'creationdate': '2024-02-19T15:15:35+08:00', 'title': '深度學習簡介', 'author': 'fhwang', 'moddate': '2024-02-19T15:15:35+08:00', 'source': '1neural_network.pdf', 'total_pages': 20, 'page': 10, 'page_label': '11'}, page_content='訓練階段(BACKWARD PROCESS)\\n11\\nx1\\nx2\\n-1\\ny1\\ny2\\n0.4\\n0.8\\n0.1\\n0.3\\n0.5\\n0.2\\nD=\\n0 1 1\\n1 0 1\\n−1 −1 −1\\nL= 1 0 1\\n1 0 0 W=\\n0.4 0.8\\n0.1 0.3\\n0.5 0.2\\n𝐻 = 𝑊𝑇 × 𝐷 = 0.4 0.1 0.5\\n0.8 0.3 0.2 ×\\n0 1 1\\n1 0 1\\n−1 −1 −1\\n= −0.4 −0.1 0\\n0.1 0.6 0.9\\n𝑌= 𝑔 𝐻 = 0 0 0\\n1 1 1\\n∆𝑊=−𝜂𝐷 𝑀+1 ×𝑁 × 𝑌𝑂×𝑁 − 𝐿𝑂×𝑁 𝑇\\n= −0.1 ∙\\n0 1 1\\n1 0 1\\n−1 −1 −1\\n×\\n−1 0\\n0 1\\n−1 1\\n= −0.1 ∙\\n−1 2\\n−2 1\\n2 −2\\n=\\n0.1 −0.2\\n0.2 −0.1\\n−0.2 0.2\\n1 1 0\\n1 0 1\\n0 0 0\\n1 1 1'),\n",
       " Document(metadata={'producer': '適用於 Microsoft 365 的 Microsoft® PowerPoint®', 'creator': '適用於 Microsoft 365 的 Microsoft® PowerPoint®', 'creationdate': '2024-02-19T15:15:35+08:00', 'title': '深度學習簡介', 'author': 'fhwang', 'moddate': '2024-02-19T15:15:35+08:00', 'source': '1neural_network.pdf', 'total_pages': 20, 'page': 11, 'page_label': '12'}, page_content='感知器的更新訓練演算法\\n12\\nStart\\nData size = N\\nMini-batch size = m\\nepochs: e\\nGenerate \\n𝑁\\n𝑚 random batches  of \\nsamples\\nNO\\nTrain with next batch\\nFinished \\nall\\nbatches?\\nNO\\nYES\\nYES\\nEnd\\nFinished \\ne\\nepochs?\\n(one epoch)'),\n",
       " Document(metadata={'producer': '適用於 Microsoft 365 的 Microsoft® PowerPoint®', 'creator': '適用於 Microsoft 365 的 Microsoft® PowerPoint®', 'creationdate': '2024-02-19T15:15:35+08:00', 'title': '深度學習簡介', 'author': 'fhwang', 'moddate': '2024-02-19T15:15:35+08:00', 'source': '1neural_network.pdf', 'total_pages': 20, 'page': 12, 'page_label': '13'}, page_content='測試階段(FORWARD PROCESS)\\n13\\n−1\\nbias\\nO\\nB   \\nWM\\nO\\n1\\nY O\\nN\\n-11\\nD\\nN\\nM\\n給定一組包含 N 個 M 輸入向量的輸入數據（D）和標籤數據（L），權重矩陣（W），\\n計算網絡的輸出數據（Y）\\nN\\nL O\\nD\\nW\\nB'),\n",
       " Document(metadata={'producer': '適用於 Microsoft 365 的 Microsoft® PowerPoint®', 'creator': '適用於 Microsoft 365 的 Microsoft® PowerPoint®', 'creationdate': '2024-02-19T15:15:35+08:00', 'title': '深度學習簡介', 'author': 'fhwang', 'moddate': '2024-02-19T15:15:35+08:00', 'source': '1neural_network.pdf', 'total_pages': 20, 'page': 13, 'page_label': '14'}, page_content='測試階段(FORWARD PROCESS)\\n• 輸入測試資料 : 𝐷(𝑀+1)×𝑁\\n• N筆 (M+1)-input 向量\\n• 加入偏值向量(-1)\\n• 輸出測試資料 : 𝐿𝑂×𝑁\\n• 權重向量(Weight Matrix)\\n• 𝑊(𝑀+1)×𝑂\\n• 加入偏值權重(B)(bias weight): 𝐵1×𝑂\\n• 網路輸出Y : 𝑌𝑂×𝑁\\n• 方差矩陣(Total Squared Error Matrix): 𝐸𝑁\\n14\\n𝑊𝑂×(𝑀+1)\\n𝑇 × 𝐷(𝑀+1)×𝑁= 𝑍𝑂×𝑁\\n𝑌𝑂×𝑁= 𝑔(𝑍𝑂×𝑁)\\n𝐸𝑁= 𝑆𝑈𝑀𝑐𝑜𝑙\\n2 (𝑌𝑂×𝑁 − 𝐿𝑂×𝑁)\\nB   \\nWM\\nO\\n1\\n-1\\nD\\nN\\nM LO\\nN\\n1\\nYO\\nN\\ng Y'),\n",
       " Document(metadata={'producer': '適用於 Microsoft 365 的 Microsoft® PowerPoint®', 'creator': '適用於 Microsoft 365 的 Microsoft® PowerPoint®', 'creationdate': '2024-02-19T15:15:35+08:00', 'title': '深度學習簡介', 'author': 'fhwang', 'moddate': '2024-02-19T15:15:35+08:00', 'source': '1neural_network.pdf', 'total_pages': 20, 'page': 14, 'page_label': '15'}, page_content='測試階段(FORWARD PROCESS)\\n15\\n𝑍 = 𝑊𝑇 × 𝐷 = 5 8 7\\n6 9 10 ×\\n1 3\\n2 4\\n−1 −1\\n= 14 32\\n14 44\\n𝑌= 𝑔 𝑍 = 1 1\\n1 1\\n𝐸𝑁= 𝑆𝑈𝑀𝑐𝑜𝑙\\n2 𝑌 − 𝐿 = 𝑆𝑈𝑀𝑐𝑜𝑙\\n2 1 1\\n1 1 − 1 3\\n2 4 = 𝑆𝑈𝑀𝑐𝑜𝑙\\n2 0 −2\\n−1 −3 = [1 13]\\n-1\\n1\\n2\\n3\\n4\\n7\\n5\\n6\\n8\\n9\\n10\\n𝐷 =\\n1 3\\n2 4\\n−1 −1\\n𝑊 =\\n5 6\\n8 9\\n7 10\\n1\\n1\\n1\\n1\\n𝐿 = 1 3\\n2 4'),\n",
       " Document(metadata={'producer': '適用於 Microsoft 365 的 Microsoft® PowerPoint®', 'creator': '適用於 Microsoft 365 的 Microsoft® PowerPoint®', 'creationdate': '2024-02-19T15:15:35+08:00', 'title': '深度學習簡介', 'author': 'fhwang', 'moddate': '2024-02-19T15:15:35+08:00', 'source': '1neural_network.pdf', 'total_pages': 20, 'page': 15, 'page_label': '16'}, page_content='測試階段(FORWARD PROCESS)\\n16\\nx1\\nx2\\n-1\\ny1\\ny2\\n0.4\\n0.8\\n0.1\\n0.3\\n0.5\\n0.2\\nD=\\n0 1 1\\n1 0 1\\n−1 −1 −1\\nL= 1 0 1\\n1 0 0 W=\\n0.4 0.8\\n0.1 0.3\\n0.5 0.2\\n𝑍 = 𝑊𝑇 × 𝐷 = 0.4 0.1 0.5\\n0.8 0.3 0.2 ×\\n0 1 1\\n1 0 1\\n−1 −1 −1\\n= −0.4 −0.1 0\\n0.1 0.6 0.9\\n𝑌= 𝑔 𝐻 = 0 0 0\\n1 1 1\\n𝐸 = 𝑆𝑈𝑀𝑐𝑜𝑙\\n2 ( 0 0 0\\n1 1 1 − 1 0 1\\n1 0 0 )=𝑆𝑈𝑀𝑐𝑜𝑙\\n2 −1 0 −1\\n0 1 1 = [1 1 2]'),\n",
       " Document(metadata={'producer': '適用於 Microsoft 365 的 Microsoft® PowerPoint®', 'creator': '適用於 Microsoft 365 的 Microsoft® PowerPoint®', 'creationdate': '2024-02-19T15:15:35+08:00', 'title': '深度學習簡介', 'author': 'fhwang', 'moddate': '2024-02-19T15:15:35+08:00', 'source': '1neural_network.pdf', 'total_pages': 20, 'page': 16, 'page_label': '17'}, page_content='矩陣運算與類神經\\n• 為什麼要以矩陣視角看待神經網路操作？\\n• 延伸至對 TensorFlow 的觀點（何謂張量(Tensor)？）\\n• 對於在晶片上（例如 GPU）進行高速計算很有用\\n• 在訓練階段，權重更新矩陣是由所有 N 筆訓練數據與原始網路權重計算收集而來\\n• 如果 N 很大會發生什麼？\\n• 如果選擇每個單一訓練數據來更新權重呢？\\n17\\nSee how matrix multiplication can be reduced to O(N) from O(N3) with parallel computation \\n(https://www.sciencedirect.com/science/article/pii/0167819189900574)'),\n",
       " Document(metadata={'producer': '適用於 Microsoft 365 的 Microsoft® PowerPoint®', 'creator': '適用於 Microsoft 365 的 Microsoft® PowerPoint®', 'creationdate': '2024-02-19T15:15:35+08:00', 'title': '深度學習簡介', 'author': 'fhwang', 'moddate': '2024-02-19T15:15:35+08:00', 'source': '1neural_network.pdf', 'total_pages': 20, 'page': 17, 'page_label': '18'}, page_content='小結\\n• 單一神經元的模型（McCulloch和Pitts 神經元）\\n• 神經元的運作方式\\n• 感知器\\n• 如何透過導出學習規則來訓練神經網路\\n• 測試感知器，看神經網絡如何進行訓練和操作\\n• 初始化權重\\n• 訓練\\n• 測試\\n• 學習將神經網路操作視為矩陣操作\\n• 學習典型的訓練過程\\n• 小批次大小\\n• 執行周期的次數\\n18'),\n",
       " Document(metadata={'producer': '適用於 Microsoft 365 的 Microsoft® PowerPoint®', 'creator': '適用於 Microsoft 365 的 Microsoft® PowerPoint®', 'creationdate': '2024-02-19T15:15:35+08:00', 'title': '深度學習簡介', 'author': 'fhwang', 'moddate': '2024-02-19T15:15:35+08:00', 'source': '1neural_network.pdf', 'total_pages': 20, 'page': 18, 'page_label': '19'}, page_content='參考文獻\\n• [1] Machine Learning: An Algorithmic Perspective, by Stephen Marsland, \\npublished by CRC Press (2014).\\n19'),\n",
       " Document(metadata={'producer': '適用於 Microsoft 365 的 Microsoft® PowerPoint®', 'creator': '適用於 Microsoft 365 的 Microsoft® PowerPoint®', 'creationdate': '2024-02-19T15:15:35+08:00', 'title': '深度學習簡介', 'author': 'fhwang', 'moddate': '2024-02-19T15:15:35+08:00', 'source': '1neural_network.pdf', 'total_pages': 20, 'page': 19, 'page_label': '20'}, page_content='Q&A\\n20')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': '適用於 Microsoft 365 的 Microsoft® PowerPoint®', 'creator': '適用於 Microsoft 365 的 Microsoft® PowerPoint®', 'creationdate': '2024-02-19T15:15:35+08:00', 'title': '深度學習簡介', 'author': 'fhwang', 'moddate': '2024-02-19T15:15:35+08:00', 'source': '1neural_network.pdf', 'total_pages': 20, 'page': 0, 'page_label': '1'}, page_content='類神經網路基礎\\n王豐緒\\n銘傳大學資工系')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the first page\n",
    "pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total pages\n",
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kenny\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "<class 'langchain_core.documents.base.Document'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "text_splitter = NLTKTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "\n",
    "chunks = text_splitter.split_documents(pages)\n",
    "\n",
    "print(len(chunks))\n",
    "\n",
    "print(type(chunks[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(google_api_key=key, model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kenny\\AppData\\Local\\Temp\\ipykernel_5428\\484647787.py:8: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  db.persist()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "db = Chroma.from_documents(chunks, embedding_model, persist_directory=\"./chroma_db_\")\n",
    "db.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kenny\\AppData\\Local\\Temp\\ipykernel_5428\\192857647.py:2: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  db_connection = Chroma(persist_directory=\"./chroma_db_\", embedding_function=embedding_model)\n"
     ]
    }
   ],
   "source": [
    "# Setting a Connection with the ChromaDB\n",
    "db_connection = Chroma(persist_directory=\"./chroma_db_\", embedding_function=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.vectorstores.base.VectorStoreRetriever'>\n"
     ]
    }
   ],
   "source": [
    "# Converting CHROMA db_connection to Retriever Object\n",
    "retriever = db_connection.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "print(type(retriever))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"\"\"You are a teacher in Scaffolding Instruction education.\n",
    "                  Given a context and question from user,\n",
    "                  you should answer based on the given context.\"\"\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"\"\"Answer the question based on the given context.\n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "    Answer: \"\"\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | chat_template\n",
    "    | chat_model\n",
    "    | output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on the context provided, a neural network, in its simplest form (Perceptron), is a network of interconnected artificial neurons inspired by biological neurons in the brain.  Each neuron receives weighted inputs (xᵢ), sums them, subtracts a threshold (θ, represented as a bias weight), and passes the result through an activation function (g) to produce an output (o or yⱼ).  Learning occurs by adjusting the connection weights (wᵢⱼ) between neurons based on the difference between the network's output (yⱼ) and the true output (tⱼ), using a learning rate (η) to control the adjustment size.  This process aims to minimize the error between predicted and actual outputs.  The provided material specifically discusses the Perceptron model, an early type of neural network with a straightforward learning rule.  More complex networks and learning algorithms exist, but the core principles remain similar.\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = rag_chain.invoke(\"\"\"Please summarize what is a neural network\"\"\")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the context provided, a neural network, in its simplest form (Perceptron), is a network of interconnected artificial neurons inspired by biological neurons in the brain.  Each neuron receives weighted inputs (xᵢ), sums them, subtracts a threshold (θ, represented as a bias weight), and passes the result through an activation function (g) to produce an output (o or yⱼ).  Learning occurs by adjusting the connection weights (wᵢⱼ) between neurons based on the difference between the network's output (yⱼ) and the true output (tⱼ), using a learning rate (η) to control the adjustment size.  This process aims to minimize the error between predicted and actual outputs.  The provided material specifically discusses the Perceptron model, an early type of neural network with a straightforward learning rule.  More complex networks and learning algorithms exist, but the core principles remain similar."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "md(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The context describes a neural network with weights represented by a matrix *W*.  Here\\'s an explanation:\\n\\n* **W (Weight Matrix):** This is the core of the network\\'s learning.  `W` is a matrix of size (M+1) x O.  \\n\\n    * M+1 represents the number of inputs to a neuron, including a bias input.\\n    * O represents the number of output neurons.\\n    * Each element `w_ij` in `W` represents the weight connecting input `i` (including the bias) to output neuron `j`.  These weights are adjusted during training to minimize error.\\n\\n* **B (Bias Weight):**  A bias term acts as an extra input to each neuron, always set to -1.  It allows the neuron to activate even when all other inputs are zero.  The bias weights are part of the weight matrix *W* in the last row.\\n\\n* **Weight Adjustment (∆W):** During the training phase (backward process), the weights in *W* are adjusted based on the difference between the network\\'s output (*Y*) and the desired output (*L*). The formula ΔW = -η * D * (Y - L)^T  describes this process:\\n\\n    *  η (eta) is the learning rate, controlling the size of the adjustments.\\n    *  D is the input data.\\n    *  Y is the network\\'s output.\\n    *  L is the target output.\\n    *  The formula calculates how much each weight contributed to the error and adjusts it accordingly.\\n\\n\\nIn essence, the weights determine the strength of the connections between the input neurons and the output neurons. The network \"learns\" by adjusting these weights so that it produces the desired output for a given input.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = rag_chain.invoke(\"\"\"Please Explain weight\"\"\")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The context describes a neural network with weights represented by a matrix *W*.  Here's an explanation:\n",
       "\n",
       "* **W (Weight Matrix):** This is the core of the network's learning.  `W` is a matrix of size (M+1) x O.  \n",
       "\n",
       "    * M+1 represents the number of inputs to a neuron, including a bias input.\n",
       "    * O represents the number of output neurons.\n",
       "    * Each element `w_ij` in `W` represents the weight connecting input `i` (including the bias) to output neuron `j`.  These weights are adjusted during training to minimize error.\n",
       "\n",
       "* **B (Bias Weight):**  A bias term acts as an extra input to each neuron, always set to -1.  It allows the neuron to activate even when all other inputs are zero.  The bias weights are part of the weight matrix *W* in the last row.\n",
       "\n",
       "* **Weight Adjustment (∆W):** During the training phase (backward process), the weights in *W* are adjusted based on the difference between the network's output (*Y*) and the desired output (*L*). The formula ΔW = -η * D * (Y - L)^T  describes this process:\n",
       "\n",
       "    *  η (eta) is the learning rate, controlling the size of the adjustments.\n",
       "    *  D is the input data.\n",
       "    *  Y is the network's output.\n",
       "    *  L is the target output.\n",
       "    *  The formula calculates how much each weight contributed to the error and adjusts it accordingly.\n",
       "\n",
       "\n",
       "In essence, the weights determine the strength of the connections between the input neurons and the output neurons. The network \"learns\" by adjusting these weights so that it produces the desired output for a given input."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown as md\n",
    "\n",
    "md(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'基於鷹架教育理念，設計一個學習類神經網路的學習路徑，可以參考以下步驟，循序漸進地引導學生理解概念並建構知識：\\n\\n**第一階段：建立基礎 - 認識單一神經元 (McCulloch-Pitts 神經元)**\\n\\n1. **具體經驗 (Concrete Experience):**  從生活中的簡單二元分類例子開始，例如：判斷圖片中的動物是否為貓。讓學生思考如何根據一些特徵（例如：是否有尖耳朵、是否有鬍鬚）進行判斷。\\n2. **觀察反思 (Reflective Observation):** 引導學生觀察 McCulloch-Pitts 神經元的結構，並與之前的分類經驗做連結。例如，將特徵對應到輸入 (x)，權重 (w) 代表每個特徵的重要性，閥值 (θ) 代表判斷的標準。\\n3. **抽象概念化 (Abstract Conceptualization):**  正式介紹 McCulloch-Pitts 神經元的數學模型，包括輸入、權重、加總、激活函數等概念。用圖表和公式解釋其運作方式，並用簡單的數值範例說明計算過程。\\n4. **主動驗證 (Active Experimentation):**  讓學生練習計算不同輸入和權重下的神經元輸出，並改變閥值觀察結果變化。可以使用線上互動工具或程式碼進行模擬。\\n\\n**第二階段：感知器 (Perceptron) - 多輸入單輸出**\\n\\n1. **具體經驗：**  延續之前的例子，增加更多特徵來判斷動物種類，例如：體型大小、毛髮顏色等。讓學生思考如何整合多個特徵進行更精確的分類。\\n2. **觀察反思：**  將感知器模型與 McCulloch-Pitts 神經元做比較，說明感知器如何處理多個輸入。\\n3. **抽象概念化：**  介紹感知器的數學模型，著重於權重向量的概念和內積運算。\\n4. **主動驗證：**  設計練習題，讓學生計算不同輸入向量和權重向量下的感知器輸出。可以使用 Python 等程式語言編寫簡單的感知器程式。\\n\\n**第三階段：訓練感知器 - 學習規則與權重更新**\\n\\n1. **具體經驗：**  以遊戲的方式模擬訓練過程，例如：讓學生扮演感知器，根據回饋調整權重來完成分類任務。\\n2. **觀察反思：**  引導學生觀察權重更新的過程，思考如何根據誤差調整權重。\\n3. **抽象概念化：**  正式介紹感知器的學習規則和權重更新公式。\\n4. **主動驗證：**  使用程式碼實現感知器的訓練過程，讓學生觀察權重如何隨著訓練迭代而變化，並將訓練結果可視化。\\n\\n**第四階段：矩陣運算與訓練過程 - 提升效率**\\n\\n1. **觀察反思：**  引導學生觀察訓練過程中大量的計算，思考如何提高效率。\\n2. **抽象概念化：**  介紹矩陣運算的概念，並說明如何用矩陣表示輸入、權重和輸出。\\n3. **主動驗證：**  使用程式碼實現基於矩陣運算的感知器訓練，並與之前的程式碼進行效能比較。\\n\\n**第五階段：測試與評估 - 檢驗學習成果**\\n\\n1. **抽象概念化：** 說明訓練集和測試集的概念，以及評估指標的重要性。\\n2. **主動驗證：** 將數據集劃分為訓練集和測試集，用訓練好的感知器對測試集進行分類，並計算準確率等評估指標。\\n\\n**第六階段：小批次大小和執行周期 - 參數調整與優化**\\n\\n1. **抽象概念化：** 介紹小批次大小和執行周期的概念，以及它們對訓練過程的影響。\\n2. **主動驗證：** 讓學生嘗試不同的參數組合，觀察訓練結果的變化，並找到最佳的參數設定。\\n\\n在整個學習過程中，教師應根據學生的學習進度和理解程度，調整鷹架的強度，逐步放手讓學生獨立探索和學習。同時，鼓勵學生之間的合作學習和互相幫助，共同建構知識。\\n\\n\\n這個學習路徑的核心是透過循序漸進的方式，從具體到抽象，從簡單到複雜，引導學生理解類神經網路的核心概念和運作原理。 並且透過不斷的練習和驗證，加深學生的理解和應用能力。'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = rag_chain.invoke(\"\"\"基於鷹架教育理念設計一個學習路徑\"\"\")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "基於鷹架教育理念，設計一個學習類神經網路的學習路徑，可以參考以下步驟，循序漸進地引導學生理解概念並建構知識：\n",
       "\n",
       "**第一階段：建立基礎 - 認識單一神經元 (McCulloch-Pitts 神經元)**\n",
       "\n",
       "1. **具體經驗 (Concrete Experience):**  從生活中的簡單二元分類例子開始，例如：判斷圖片中的動物是否為貓。讓學生思考如何根據一些特徵（例如：是否有尖耳朵、是否有鬍鬚）進行判斷。\n",
       "2. **觀察反思 (Reflective Observation):** 引導學生觀察 McCulloch-Pitts 神經元的結構，並與之前的分類經驗做連結。例如，將特徵對應到輸入 (x)，權重 (w) 代表每個特徵的重要性，閥值 (θ) 代表判斷的標準。\n",
       "3. **抽象概念化 (Abstract Conceptualization):**  正式介紹 McCulloch-Pitts 神經元的數學模型，包括輸入、權重、加總、激活函數等概念。用圖表和公式解釋其運作方式，並用簡單的數值範例說明計算過程。\n",
       "4. **主動驗證 (Active Experimentation):**  讓學生練習計算不同輸入和權重下的神經元輸出，並改變閥值觀察結果變化。可以使用線上互動工具或程式碼進行模擬。\n",
       "\n",
       "**第二階段：感知器 (Perceptron) - 多輸入單輸出**\n",
       "\n",
       "1. **具體經驗：**  延續之前的例子，增加更多特徵來判斷動物種類，例如：體型大小、毛髮顏色等。讓學生思考如何整合多個特徵進行更精確的分類。\n",
       "2. **觀察反思：**  將感知器模型與 McCulloch-Pitts 神經元做比較，說明感知器如何處理多個輸入。\n",
       "3. **抽象概念化：**  介紹感知器的數學模型，著重於權重向量的概念和內積運算。\n",
       "4. **主動驗證：**  設計練習題，讓學生計算不同輸入向量和權重向量下的感知器輸出。可以使用 Python 等程式語言編寫簡單的感知器程式。\n",
       "\n",
       "**第三階段：訓練感知器 - 學習規則與權重更新**\n",
       "\n",
       "1. **具體經驗：**  以遊戲的方式模擬訓練過程，例如：讓學生扮演感知器，根據回饋調整權重來完成分類任務。\n",
       "2. **觀察反思：**  引導學生觀察權重更新的過程，思考如何根據誤差調整權重。\n",
       "3. **抽象概念化：**  正式介紹感知器的學習規則和權重更新公式。\n",
       "4. **主動驗證：**  使用程式碼實現感知器的訓練過程，讓學生觀察權重如何隨著訓練迭代而變化，並將訓練結果可視化。\n",
       "\n",
       "**第四階段：矩陣運算與訓練過程 - 提升效率**\n",
       "\n",
       "1. **觀察反思：**  引導學生觀察訓練過程中大量的計算，思考如何提高效率。\n",
       "2. **抽象概念化：**  介紹矩陣運算的概念，並說明如何用矩陣表示輸入、權重和輸出。\n",
       "3. **主動驗證：**  使用程式碼實現基於矩陣運算的感知器訓練，並與之前的程式碼進行效能比較。\n",
       "\n",
       "**第五階段：測試與評估 - 檢驗學習成果**\n",
       "\n",
       "1. **抽象概念化：** 說明訓練集和測試集的概念，以及評估指標的重要性。\n",
       "2. **主動驗證：** 將數據集劃分為訓練集和測試集，用訓練好的感知器對測試集進行分類，並計算準確率等評估指標。\n",
       "\n",
       "**第六階段：小批次大小和執行周期 - 參數調整與優化**\n",
       "\n",
       "1. **抽象概念化：** 介紹小批次大小和執行周期的概念，以及它們對訓練過程的影響。\n",
       "2. **主動驗證：** 讓學生嘗試不同的參數組合，觀察訓練結果的變化，並找到最佳的參數設定。\n",
       "\n",
       "在整個學習過程中，教師應根據學生的學習進度和理解程度，調整鷹架的強度，逐步放手讓學生獨立探索和學習。同時，鼓勵學生之間的合作學習和互相幫助，共同建構知識。\n",
       "\n",
       "\n",
       "這個學習路徑的核心是透過循序漸進的方式，從具體到抽象，從簡單到複雜，引導學生理解類神經網路的核心概念和運作原理。 並且透過不斷的練習和驗證，加深學生的理解和應用能力。"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown as md\n",
    "\n",
    "md(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
